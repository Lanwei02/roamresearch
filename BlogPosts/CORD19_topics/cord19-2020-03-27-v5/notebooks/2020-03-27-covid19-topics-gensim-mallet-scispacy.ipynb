{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics Modeling using Mallet (through gensim wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "#!pip install panel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension()  # This can cause Save to error \"Requested Entity to large\"; Clear this cell's output after running\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALLET_ROOT = '/home/jovyan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_home = os.path.join(MALLET_ROOT, 'mallet-2.0.8')\n",
    "mallet_path = os.path.join(mallet_home, 'bin', 'mallet')\n",
    "mallet_stoplist_path = os.path.join(mallet_home, 'stoplists', 'en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "datafile_date = '2020-03-27-v5'\n",
    "basedir = ROOT + f'/data/interim/{datafile_date}/'\n",
    "# parser = 'moana'\n",
    "parser = 'scispacy'\n",
    "parser_model = 'spacy-en_core_sci_lg'\n",
    "# Inputs\n",
    "datafile = f'{basedir}{datafile_date}-covid19-combined-abstracts-tokens-{parser_model}.jsonl'\n",
    "text_column_name = 'abstract_clean'\n",
    "tokens_column_name = f'abstract_tokens_{parser}'\n",
    "ent_column_name = f'abstract_ent_{parser}'\n",
    "json_args = {'orient': 'records', 'lines': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs\n",
    "outdir = ROOT + f'/results/{datafile_date}/'\n",
    "model_out_dir = ROOT + f'/models/topics-abstracts-{datafile_date}-{parser}/'\n",
    "model_path = model_out_dir + 'mallet_models/'\n",
    "gs_model_path = model_path + 'gs_models/'\n",
    "gs_model_path_prefix = gs_model_path + f'{datafile_date}-covid19-combined-abstracts-'\n",
    "out_json_args = {'date_format': 'iso', **json_args}\n",
    "web_out_dir = outdir + f'topics-abstracts-{datafile_date}-{parser}-html/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(datafile):\n",
    "    print(datafile + ' does not exist')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_mode = 0o777\n",
    "os.makedirs(model_out_dir, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(model_path, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(gs_model_path, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(outdir, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(web_out_dir, mode = out_path_mode, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(mallet_stoplist_path, 'r') as fp:\n",
    "    stopwords = set(fp.read().split())\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.update([\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.',\n",
    "    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n",
    "])    # from https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in text and create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_json(datafile, **json_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = original_df[text_column_name]\n",
    "orig_tokens = original_df[tokens_column_name]\n",
    "if ent_column_name in original_df.columns:\n",
    "    ents = original_df['abstract_ent_scispacy'].apply(lambda lst: ['_'.join(k.lower().split()) for k in lst if len(k.split()) > 1])\n",
    "else:\n",
    "    ents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31753"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation + \"”“–\"  # remove both slanted double-quotes\n",
    "# leave '#$%*+-/<=>'\n",
    "nonnumeric_punctuation = r'!\"&()\\,.:;?@[]^_`{|}~' + \"'\" + \"'\"\"”“–’\" + ' '\n",
    "\n",
    "def normalize_token(token):\n",
    "    if token in nonnumeric_punctuation:\n",
    "        return None\n",
    "    if token in stopwords:\n",
    "        return None\n",
    "    if token == token.upper():\n",
    "        return token\n",
    "    return token.lower()\n",
    "\n",
    "def normalize_token_list(tokens):\n",
    "    result = []\n",
    "    for tok in tokens:\n",
    "        ntok = normalize_token(tok)\n",
    "        if ntok:\n",
    "            result.append(ntok)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = orig_tokens.apply(normalize_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', '$', '%', \"'s\", '($2196.8)/case']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dictionary.values())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model collections -- vary corpus and n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare corpus collections (various options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "# corpora['text'] = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter scispacy ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "if ents is not None:\n",
    "    ents_counter = Counter()\n",
    "    for x in ents.iteritems():\n",
    "        for w in x[1]:\n",
    "            ents_counter[w] += 1\n",
    "    ents_common = [k for k, c in ents_counter.items() if c >= 5]\n",
    "    len(ents_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extended token sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "if ents is not None:\n",
    "    dictionary.add_documents([ents_common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several combinations attempted, but 'text-ents' was most useful\n",
    "if ents is not None:\n",
    "    corpora['text-ents'] = (texts + ents).apply(dictionary.doc2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text-ents'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_template = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<meta charset=\"UTF-8\">\n",
    "<head>\n",
    "  <title>{0}</title>\n",
    "{1}\n",
    "</head>\n",
    "<body>\n",
    "<h2>{0}</h2>\n",
    "{2}\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "html_style = '''\n",
    "<style>\n",
    "table {\n",
    "  font-family: \"Trebuchet MS\", Arial, Helvetica, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #ddd;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even){background-color: #f2f2f2;}\n",
    "\n",
    "tr:hover {background-color: #ddd;}\n",
    "\n",
    "th {\n",
    "  padding-top: 12px;\n",
    "  padding-bottom: 12px;\n",
    "  text-align: left;\n",
    "  background-color: #0099FF;\n",
    "  color: white;\n",
    "}\n",
    "</style>\n",
    "'''\n",
    "\n",
    "html_style_cols = '''\n",
    "<style>\n",
    "table {\n",
    "  font-family: \"Trebuchet MS\", Arial, Helvetica, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #ddd;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "td:nth-child(even){background-color: #f2f2f2;}\n",
    "\n",
    "td:hover {background-color: #ddd;}\n",
    "\n",
    "th {\n",
    "  padding-top: 12px;\n",
    "  padding-bottom: 12px;\n",
    "  text-align: left;\n",
    "  background-color: #0099FF;\n",
    "  color: white;\n",
    "}\n",
    "</style>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = [60]  # number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for text-ents (60 topic)\n"
     ]
    }
   ],
   "source": [
    "cmallet = {}\n",
    "for c in corpora.keys():\n",
    "    cmallet[c] = {}\n",
    "    for i in num_topics:\n",
    "        print('Building model for %s (%s topic)' % (c,i))\n",
    "        prefix = os.path.join(model_path, c, str(i), '')\n",
    "        os.makedirs(prefix, mode = out_path_mode, exist_ok = True)\n",
    "        cmallet[c][i] = gensim.models.wrappers.ldamallet.LdaMallet(mallet_path, corpora[c], id2word=dictionary, optimize_interval=10,\n",
    "                                                            prefix=prefix, \n",
    "                                                                   num_topics=i, iterations='2500 --random-seed 42')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:223: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
      "/opt/conda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:240: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lift = np.log(topic_term_dists / term_proportion)\n",
      "/opt/conda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:241: RuntimeWarning: divide by zero encountered in log\n",
      "  log_ttd = np.log(topic_term_dists)\n"
     ]
    }
   ],
   "source": [
    "vis_data = {}\n",
    "gensim_lda_model = {}\n",
    "for c in cmallet.keys():\n",
    "    vis_data[c] = {}\n",
    "    gensim_lda_model[c] = {}\n",
    "    for i in cmallet[c].keys():\n",
    "        gensim_lda_model[c][i] = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(cmallet[c][i])\n",
    "        vis_data[c][i] = pyLDAvis.gensim.prepare(gensim_lda_model[c][i], corpora[c], \n",
    "                                                   dictionary=cmallet[c][i].id2word, mds='tsne')\n",
    "        pyLDAvis.save_html(vis_data[c][i], outdir + f'pyldavis_{c}_{i}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Gensim Mallet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/topics-abstracts-2020-03-27-v5-scispacy/mallet_models/gs_models/2020-03-27-v5-covid19-combined-abstracts-gensim-mallet-model_text-ents_60.pickle4\n"
     ]
    }
   ],
   "source": [
    "for c in gensim_lda_model.keys():\n",
    "    for i in gensim_lda_model[c].keys():\n",
    "        gensim_lda_model[c][i].save(f'{gs_model_path_prefix}gensim-mallet-model_{c}_{i}.pickle4', \n",
    "                                    separately=[], sep_limit=134217728, pickle_protocol=4)\n",
    "        print(f'{gs_model_path_prefix}gensim-mallet-model_{c}_{i}.pickle4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save _Relevant_ terms for topics (from pyLDAviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_terms = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_terms(data, topic=1, rlambda=1, num_terms=30):\n",
    "    \"\"\"Returns a dataframe using lambda to calculate term relevance of a given topic.\"\"\"\n",
    "    tdf = pd.DataFrame(data.topic_info[data.topic_info.Category == 'Topic' + str(topic)])\n",
    "    if rlambda < 0 or rlambda > 1:\n",
    "        rlambda = 1\n",
    "    stdf = tdf.assign(relevance=rlambda * tdf['logprob'] + (1 - rlambda) * tdf['loglift'])\n",
    "    rdf = stdf[['Term', 'relevance']]\n",
    "    if num_terms:\n",
    "        return rdf.sort_values('relevance', ascending=False).head(num_terms).set_index(['Term'])\n",
    "    else:\n",
    "        return rdf.sort_values('relevance', ascending=False).set_index(['Term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lists = {}\n",
    "for corp, cdict in vis_data.items():\n",
    "    for numtops in cdict.keys():\n",
    "        model_topic_lists_dict = {}\n",
    "        for topnum in range(numtops):\n",
    "            s = sorted_terms(vis_data[corp][numtops], topnum + 1, rlambda=.5, num_terms=num_terms)\n",
    "            terms = s.index\n",
    "            model_topic_lists_dict['Topic ' + str(topnum + 1)] = np.pad(terms, (0, num_terms - len(terms)),\n",
    "                                                                               'constant', constant_values='')\n",
    "        topic_lists[corp + '-' + str(numtops)] = pd.DataFrame(model_topic_lists_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text-ents-60'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_lists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/topics-relevant-words-abstracts-2020-03-27-v5-50terms.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save relevant topics - write to xlsx (one corp-numtopics per sheet)\n",
    "with pd.ExcelWriter(outdir + f'topics-relevant-words-abstracts-{datafile_date}-{num_terms}terms.xlsx') as writer: \n",
    "    for sheetname, dataframe in topic_lists.items():\n",
    "        dataframe.to_excel(writer, sheet_name=sheetname)\n",
    "print(outdir + f'topics-relevant-words-abstracts-{datafile_date}-{num_terms}terms.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Relevant Topics as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/topics-abstracts-2020-03-27-v5-scispacy-html/text-ents-60/relevant_terms.html\n"
     ]
    }
   ],
   "source": [
    "# Save relevant topics - write to html\n",
    "out_topics_html_dir = web_out_dir\n",
    "for corp_numtopics, dataframe in topic_lists.items():\n",
    "    os.makedirs(out_topics_html_dir + corp_numtopics, mode = out_path_mode, exist_ok = True)\n",
    "    ofname = out_topics_html_dir + corp_numtopics + '/' + 'relevant_terms.html'\n",
    "    with open(ofname, 'w') as ofp:\n",
    "        column_tags = [f'<a href=\"Topic_{i+1:02d}.html\" target=\"_blank\">{name}</a>' \n",
    "                       for i, name in enumerate(dataframe.columns)]\n",
    "        temp_df = dataframe.copy()\n",
    "        temp_df.columns = column_tags\n",
    "        temp_df = temp_df.applymap(lambda x: ' '.join(x.split('_')))\n",
    "        temp_df = temp_df.set_index(np.arange(1, len(temp_df) + 1))\n",
    "        html_table = temp_df.to_html(escape=False)\n",
    "        html_str = html_template.format('Most Relevant Terms per Topic', html_style_cols, html_table)\n",
    "        ofp.write(html_str)\n",
    "    print(ofname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# topic_lists['text-ents-60']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes of topic model collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctopicwords_df = {}\n",
    "for c in cmallet.keys():\n",
    "    ctopicwords_df[c] = {}\n",
    "    for i in cmallet[c].keys():\n",
    "        ctopicwords_df[c][i] = pd.read_table(cmallet[c][i].ftopickeys(), header=None, names=['id', 'weight', 'wordlist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVED = []\n",
    "def normalize_topic_words(words):\n",
    "    results = []\n",
    "    for w in words:\n",
    "        if w in nonnumeric_punctuation:\n",
    "            pass\n",
    "        elif w[-1] == 's' and w[:-1] in words:\n",
    "            # remove plural\n",
    "            REMOVED.append(w)\n",
    "        elif w != w.lower() and w.lower() in words:\n",
    "            # remove capitalized\n",
    "            REMOVED.append(w)\n",
    "        else:\n",
    "            results.append(w)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean words\n",
    "for c in ctopicwords_df.keys():\n",
    "    for i in ctopicwords_df[c].keys():\n",
    "        ctopicwords_df[c][i]['wordlist'] = ctopicwords_df[c][i]['wordlist'].apply(lambda x: ' '.join(normalize_topic_words(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(REMOVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ctopicwords_df.keys():\n",
    "    for i in ctopicwords_df[c].keys():\n",
    "        ctopicwords_df[c][i].drop(['id'], axis=1, inplace=True)\n",
    "        ctopicwords_df[c][i]['topwords'] = ctopicwords_df[c][i].wordlist.apply(lambda x: ' '.join(x.split()[:3]))\n",
    "        ctopicwords_df[c][i]['topten'] = ctopicwords_df[c][i].wordlist.apply(lambda x: ' '.join(x.split()[:10]))\n",
    "        if True:  # use pyLDAvis order\n",
    "            rank_order_new_old = vis_data[c][i].to_dict()['topic.order']\n",
    "            rank_order_old_new = [None] * len(rank_order_new_old)\n",
    "            for new, old in enumerate(rank_order_new_old):\n",
    "                rank_order_old_new[old - 1] = new\n",
    "            ctopicwords_df[c][i]['rank'] = np.array(rank_order_old_new) + 1\n",
    "        else:\n",
    "            ctopicwords_df[c][i]['rank'] = ctopicwords_df[c][i].weight.rank(ascending=False)\n",
    "        ctopicwords_df[c][i]['topicnum'] = ctopicwords_df[c][i].apply(lambda row: ('t%02d' % row['rank']), axis=1)\n",
    "        ctopicwords_df[c][i]['label'] = ctopicwords_df[c][i].apply(lambda row: row['topicnum'] + ' ' + row['topwords'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doctopics\n",
    "cdoctopics_df = {}\n",
    "for c in cmallet.keys():\n",
    "    cdoctopics_df[c] = {}\n",
    "    for n in cmallet[c].keys():\n",
    "        cdoctopics_df[c][n] = pd.read_table(cmallet[c][n].fdoctopics(), header=None, names=['id']+[i for i in range(n)])\n",
    "        cdoctopics_df[c][n].drop(['id'], axis=1, inplace=True)\n",
    "# cdoctopics_df[c][n].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic</th>\n",
       "      <th>t01</th>\n",
       "      <th>t02</th>\n",
       "      <th>t03</th>\n",
       "      <th>t04</th>\n",
       "      <th>t05</th>\n",
       "      <th>t06</th>\n",
       "      <th>t07</th>\n",
       "      <th>t08</th>\n",
       "      <th>t09</th>\n",
       "      <th>t10</th>\n",
       "      <th>...</th>\n",
       "      <th>t51</th>\n",
       "      <th>t52</th>\n",
       "      <th>t53</th>\n",
       "      <th>t54</th>\n",
       "      <th>t55</th>\n",
       "      <th>t56</th>\n",
       "      <th>t57</th>\n",
       "      <th>t58</th>\n",
       "      <th>t59</th>\n",
       "      <th>t60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.105870</td>\n",
       "      <td>0.017820</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.005241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.028607</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.006219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.028431</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.047748</td>\n",
       "      <td>0.031532</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.020175</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.162281</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.004386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "topic       t01       t02       t03       t04       t05       t06       t07  \\\n",
       "0      0.005241  0.005241  0.005241  0.005241  0.005241  0.005241  0.005241   \n",
       "1      0.006219  0.006219  0.006219  0.013682  0.021144  0.006219  0.028607   \n",
       "2      0.004902  0.004902  0.004902  0.004902  0.004902  0.004902  0.004902   \n",
       "3      0.004505  0.004505  0.004505  0.004505  0.004505  0.015315  0.047748   \n",
       "4      0.009649  0.020175  0.009649  0.009649  0.004386  0.004386  0.162281   \n",
       "\n",
       "topic       t08       t09       t10  ...       t51       t52       t53  \\\n",
       "0      0.105870  0.017820  0.005241  ...  0.005241  0.005241  0.005241   \n",
       "1      0.006219  0.006219  0.013682  ...  0.006219  0.006219  0.013682   \n",
       "2      0.022549  0.004902  0.004902  ...  0.004902  0.010784  0.028431   \n",
       "3      0.031532  0.009910  0.004505  ...  0.004505  0.004505  0.004505   \n",
       "4      0.004386  0.004386  0.004386  ...  0.004386  0.004386  0.004386   \n",
       "\n",
       "topic       t54       t55       t56       t57       t58       t59       t60  \n",
       "0      0.005241  0.005241  0.005241  0.005241  0.005241  0.005241  0.005241  \n",
       "1      0.006219  0.013682  0.006219  0.006219  0.006219  0.006219  0.006219  \n",
       "2      0.004902  0.004902  0.004902  0.004902  0.040196  0.004902  0.004902  \n",
       "3      0.004505  0.004505  0.004505  0.004505  0.004505  0.004505  0.004505  \n",
       "4      0.004386  0.009649  0.004386  0.004386  0.004386  0.009649  0.004386  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder topics\n",
    "for c in cdoctopics_df.keys():\n",
    "    for n in cdoctopics_df[c].keys():\n",
    "# (include top 3 topics in name)        cdoctopics_df[c][n] = cdoctopics_df[c][n].T.join(ctopicwords_df[c][n][['rank', 'label']]).set_index('label').sort_values('rank').drop(['rank'], axis=1).T\n",
    "        cdoctopics_df[c][n] = cdoctopics_df[c][n].T.join(ctopicwords_df[c][n][['rank', 'topicnum']]).set_index('topicnum').sort_values('rank').drop(['rank'], axis=1).T\n",
    "        cdoctopics_df[c][n].T.index.rename('topic', inplace=True)\n",
    "cdoctopics_df[c][n].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/topickeys_sorted_text-ents_60.txt\n"
     ]
    }
   ],
   "source": [
    "# Save topicwords\n",
    "for c in ctopicwords_df.keys():\n",
    "    for i in ctopicwords_df[c].keys():\n",
    "        ctopicwords_df[c][i].sort_values('rank').to_csv(outdir + 'topickeys_sorted_%s_%d.txt' % (c, i), index_label='original_order')\n",
    "        print(outdir + 'topickeys_sorted_%s_%d.txt' % (c, i))\n",
    "        # ctopicwords_df[c][i].sort_values('rank').to_excel('out/topickeys_sorted_%s_%d.xlsx' % (c, i), index_label='original_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/doctopic_text-ents_60.csv\n"
     ]
    }
   ],
   "source": [
    "# Save doctopics\n",
    "for c in cdoctopics_df.keys():\n",
    "    for n in cdoctopics_df[c].keys():\n",
    "        cdoctopics_df[c][n].to_csv(outdir + 'doctopic_%s_%d.csv' % (c, n), index_label='original_order')\n",
    "        print(outdir + 'doctopic_%s_%d.csv' % (c, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_covid19_regex = re.compile('covid-19|sars-cov-2|2019-ncov|sars coronavirus 2|2019 novel coronavirus',\n",
    "                                re.IGNORECASE)\n",
    "def match_covid19(text):\n",
    "    return bool(match_covid19_regex.match(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to save docs by topics\n",
    "predominant_doc_dfd = {}\n",
    "predominant_doc_df = original_df[['cite_ad', 'title', 'authors', 'publish_year', 'publish_time', 'dataset',\n",
    "                                 'pmcid', 'pubmed_id', 'doi', 'sha', 'abstract_clean']].copy()\n",
    "predominant_doc_df['mentions_COVID-19'] = predominant_doc_df['abstract_clean'].apply(match_covid19)\n",
    "predominant_doc_df['publish_time'] = predominant_doc_df['publish_time'].dt.strftime('%Y-%m-%d')\n",
    "for c in cdoctopics_df.keys():\n",
    "    predominant_doc_dfd[c] = {}\n",
    "    for n in cdoctopics_df[c].keys():\n",
    "        predominant_doc_dfd[c][n] = {}\n",
    "        predominant_doc_df['major_topics'] = cdoctopics_df[c][n].apply(lambda r: {f't{i + 1:02d}': val for i, val in enumerate(r) if val >= 0.3}, axis=1)\n",
    "        for i, topic_name in enumerate(cdoctopics_df[c][n].columns):        \n",
    "            temp_df = predominant_doc_df[(predominant_doc_df['major_topics'].apply(lambda x: topic_name in x))].copy()\n",
    "            temp_df['topic_weight'] = temp_df.major_topics.apply(lambda x: x.get(topic_name))\n",
    "            temp_df = temp_df.sort_values(['topic_weight'], axis=0, ascending=False)\n",
    "            predominant_doc_dfd[c][n][i] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/topics-central-docs-abstracts-2020-03-27-v5-text-ents-60.{jsonl, txt}\n"
     ]
    }
   ],
   "source": [
    "# Save docs by topics - write to json and tsv\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        outfile_central_docs_base = outdir + f'topics-central-docs-abstracts-{datafile_date}-{c}-{n}'\n",
    "        temp_dfs = []\n",
    "        for i, dataframe in predominant_doc_dfd[c][n].items():\n",
    "            temp_df = dataframe[['title', 'authors', 'publish_year', 'publish_time', 'dataset', 'sha', 'abstract_clean']].reset_index()\n",
    "            temp_df['Topic'] = i + 1\n",
    "            temp_dfs.append(temp_df)\n",
    "        result_df = pd.concat(temp_dfs)\n",
    "        print(outfile_central_docs_base + '.{jsonl, txt}')\n",
    "        result_df.to_json(outfile_central_docs_base + '.jsonl', **out_json_args)\n",
    "        result_df.to_csv(outfile_central_docs_base + '.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/topics-central-docs-abstracts-2020-03-27-v5-text-ents-60.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save docs by topics - write to excel\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        print(outdir + f'topics-central-docs-abstracts-{datafile_date}-{c}-{n}.xlsx')\n",
    "        with pd.ExcelWriter(outdir + f'topics-central-docs-abstracts-{datafile_date}-{c}-{n}.xlsx') as writer: \n",
    "            for i in predominant_doc_dfd[c][n].keys():\n",
    "                sheetname = f'Topic {i+1}'\n",
    "                predominant_doc_dfd[c][n][i].drop(columns=['abstract_clean', 'cite_ad', 'major_topics']).to_excel(writer, sheet_name=sheetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for html\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        for i in predominant_doc_dfd[c][n].keys():\n",
    "            predominant_doc_dfd[c][n][i]['pmcid'] = predominant_doc_dfd[c][n][i]['pmcid'].apply(lambda xid: f'<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/{xid}\" target=\"_blank\">{xid}</a>' if not pd.isnull(xid) else '')\n",
    "            predominant_doc_dfd[c][n][i]['pubmed_id'] = predominant_doc_dfd[c][n][i]['pubmed_id'].apply(lambda xid: f'<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/{xid}\" target=\"_blank\">{xid}</a>' if not pd.isnull(xid) else '')\n",
    "            predominant_doc_dfd[c][n][i]['doi'] = predominant_doc_dfd[c][n][i]['doi'].apply(lambda xid: f'<a href=\"https://doi.org/{xid}\" target=\"_blank\">{xid}</a>' if not pd.isnull(xid) else '')\n",
    "            predominant_doc_dfd[c][n][i].columns = [' '.join(c.split('_')) for c in predominant_doc_dfd[c][n][i].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-03-27-v5/topics-central-docs-abstracts-2020-03-27-v5-html/text-ents-60/\n"
     ]
    }
   ],
   "source": [
    "# Save doc by topics - write to html\n",
    "out_topics_html_dir = outdir + f'topics-central-docs-abstracts-{datafile_date}-html/'\n",
    "os.makedirs(out_topics_html_dir, mode = out_path_mode, exist_ok = True)\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        ofdir = out_topics_html_dir + f'{c}-{n}/'\n",
    "        os.makedirs(ofdir, mode = out_path_mode, exist_ok = True)   \n",
    "        print(ofdir)\n",
    "        for i in predominant_doc_dfd[c][n].keys():\n",
    "            ofname = ofdir + f'Topic_{i+1:02d}.html'\n",
    "            with open(ofname, 'w') as ofp:\n",
    "                html_table = (predominant_doc_dfd[c][n][i]\n",
    "                                .drop(columns=['cite ad', 'sha', 'major topics', 'abstract clean'])\n",
    "                                .copy()\n",
    "                                .set_index(np.arange(1, len(predominant_doc_dfd[c][n][i])+1))\n",
    "                                .to_html(escape=False))\n",
    "                html_str = html_template.format(f'Topic {i+1:02d}', html_style, html_table)\n",
    "                ofp.write(html_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
