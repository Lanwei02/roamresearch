{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics Modeling using Mallet (through gensim wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "#!pip install panel\n",
    "#!pip install pycld2\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import panel as pn\n",
    "import pycld2 as cld2\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension()  # This can cause Save to error \"Requested Entity to large\"; Clear this cell's output after running\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALLET_ROOT = '/home/jovyan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_home = os.path.join(MALLET_ROOT, 'mark/Systems/mallet-2.0.8')\n",
    "mallet_path = os.path.join(mallet_home, 'bin', 'mallet')\n",
    "mallet_stoplist_path = os.path.join(mallet_home, 'stoplists', 'en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "datafile_date = '2020-04-10-v7'\n",
    "basedir = ROOT + f'/data/interim/{datafile_date}/'\n",
    "# parser = 'moana'\n",
    "parser = 'scispacy'\n",
    "parser_model = 'spacy-en_core_sci_lg'\n",
    "# Inputs\n",
    "datafile = f'{basedir}{datafile_date}-covid19-combined-abstracts-tokens-{parser_model}.jsonl'\n",
    "text_column_name = 'abstract_clean'\n",
    "tokens_column_name = f'abstract_tokens_{parser}'\n",
    "ent_column_name = f'abstract_ent_{parser}'\n",
    "json_args = {'orient': 'records', 'lines': True}\n",
    "# Other configurations\n",
    "MODIFIED_LDAVIS_URL = 'https://cdn.jsdelivr.net/gh/roamanalytics/roamresearch@master/BlogPosts/CORD19_topics/ldavis.v1.0.0-roam.js'\n",
    "random_seed = 42\n",
    "model_build_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs\n",
    "outdir = ROOT + f'/results/{datafile_date}/'\n",
    "model_out_dir = ROOT + f'/models/topics-abstracts-{datafile_date}-{parser}/'\n",
    "model_path = model_out_dir + 'mallet_models/'\n",
    "gs_model_path = model_path + 'gs_models/'\n",
    "gs_model_path_prefix = gs_model_path + f'{datafile_date}-covid19-combined-abstracts-'\n",
    "out_json_args = {'date_format': 'iso', **json_args}\n",
    "web_out_dir = outdir + f'topics-abstracts-{datafile_date}-{parser}-html/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(datafile):\n",
    "    print(datafile + ' does not exist')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_mode = 0o777\n",
    "os.makedirs(model_out_dir, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(model_path, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(gs_model_path, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(outdir, mode = out_path_mode, exist_ok = True)\n",
    "os.makedirs(web_out_dir, mode = out_path_mode, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(mallet_stoplist_path, 'r') as fp:\n",
    "    stopwords = set(fp.read().split())\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.update([\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.',\n",
    "    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n",
    "])    # from https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in text and create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_json(datafile, **json_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = original_df[text_column_name]\n",
    "orig_tokens = original_df[tokens_column_name]\n",
    "if 'keyterms' in original_df.columns:\n",
    "    # keyterms = original_df['keyterms'].apply(lambda x: [k.lower() for k in x])\n",
    "    keyterms = original_df['keyterms'].apply(lambda lst: ['_'.join(k.lower().split()) for k in lst])\n",
    "else:\n",
    "    keyterms = None\n",
    "if ent_column_name in original_df.columns:\n",
    "    ents = original_df['abstract_ent_scispacy'].apply(lambda lst: ['_'.join(k.lower().split()) for k in lst if len(k.split()) > 1])\n",
    "else:\n",
    "    ents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38022"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation + \"”“–\"  # remove both slanted double-quotes\n",
    "# leave '#$%*+-/<=>'\n",
    "nonnumeric_punctuation = r'!\"&()\\,.:;?@[]^_`{|}~' + \"'\" + \"'\"\"”“–’\" + ' '\n",
    "\n",
    "def normalize_token(token):\n",
    "    if token in nonnumeric_punctuation:\n",
    "        return None\n",
    "    if token in stopwords:\n",
    "        return None\n",
    "    if token == token.upper():\n",
    "        return token\n",
    "    return token.lower()\n",
    "\n",
    "def normalize_token_list(tokens):\n",
    "    result = []\n",
    "    for tok in tokens:\n",
    "        ntok = normalize_token(tok)\n",
    "        if ntok:\n",
    "            result.append(ntok)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"&()\\\\,.:;?@[]^_`{|}~\\'\\'”“–’ '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonnumeric_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = orig_tokens.apply(normalize_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', '$', '%', \"'s\", \"'ve\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dictionary.values())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model collections -- vary corpus and n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare corpus collections (various options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "# corpora['text'] = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(text):\n",
    "    try:\n",
    "        isReliable, _, details = cld2.detect(text, isPlainText=True)\n",
    "    except cld2.error:\n",
    "        return ('ERROR', 0, '')\n",
    "    if isReliable:\n",
    "        lang_prob = details[0][2]\n",
    "        if lang_prob > 70:\n",
    "            return (details[0][1], lang_prob, text)\n",
    "        elif lang_prob == 0:\n",
    "            return ('', 0, '')\n",
    "        # abstract likely in two languages\n",
    "        _, _, details, vectors = cld2.detect(text, isPlainText=True,\n",
    "                                             returnVectors=True, hintLanguage='en')\n",
    "        en_text = ''\n",
    "        for vec in vectors:\n",
    "            if vec[3] == 'en':\n",
    "                en_text += text[vec[0] : vec[0]+vec[1]]\n",
    "        return ('en-extract', lang_prob, en_text)\n",
    "    else:\n",
    "        return ('', 0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lang = pd.DataFrame.from_records(documents.apply(predict_language), columns=('lang', 'lang_prob', 'text'), index=documents.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4595"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lang_en_mask = predicted_lang['lang'].isin(['en', 'en-extract'])\n",
    "(~ predicted_lang_en_mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_en = texts.where(predicted_lang_en_mask, None)\n",
    "texts_en = texts.apply(lambda x: x if x is not None else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter scispacy ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "if ents is not None:\n",
    "    ents_counter = Counter()\n",
    "    for x in ents.iteritems():\n",
    "        for w in x[1]:\n",
    "            ents_counter[w] += 1\n",
    "    ents_common = [k for k, c in ents_counter.items() if c >= 5]\n",
    "    len(ents_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extended token sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "if ents is not None:\n",
    "    dictionary.add_documents([ents_common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several combinations attempted, but 'text-ents' was most useful\n",
    "if ents is not None:\n",
    "#    corpora['text-ents'] = (texts + ents).apply(dictionary.doc2bow)\n",
    "    corpora['text-ents-en'] = (texts_en + ents).apply(dictionary.doc2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text-ents-en'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_template = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<meta charset=\"UTF-8\">\n",
    "<head>\n",
    "  <title>{0}</title>\n",
    "{1}\n",
    "</head>\n",
    "<body>\n",
    "<h2>{0}</h2>\n",
    "{2}\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "html_style = '''\n",
    "<style>\n",
    "table {\n",
    "  font-family: \"Trebuchet MS\", Arial, Helvetica, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #ddd;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even){background-color: #f2f2f2;}\n",
    "\n",
    "tr:hover {background-color: #ddd;}\n",
    "\n",
    "th {\n",
    "  padding-top: 12px;\n",
    "  padding-bottom: 12px;\n",
    "  text-align: left;\n",
    "  background-color: #0099FF;\n",
    "  color: white;\n",
    "}\n",
    "</style>\n",
    "'''\n",
    "\n",
    "html_style_cols = '''\n",
    "<style>\n",
    "table {\n",
    "  font-family: \"Trebuchet MS\", Arial, Helvetica, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #ddd;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "td:nth-child(even){background-color: #f2f2f2;}\n",
    "\n",
    "td:hover {background-color: #ddd;}\n",
    "\n",
    "th {\n",
    "  padding-top: 12px;\n",
    "  padding-bottom: 12px;\n",
    "  text-align: left;\n",
    "  background-color: #0099FF;\n",
    "  color: white;\n",
    "}\n",
    "</style>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = [80]  # number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for text-ents-en (80 topic)\n"
     ]
    }
   ],
   "source": [
    "cmallet = {}\n",
    "for c in corpora.keys():\n",
    "    cmallet[c] = {}\n",
    "    for i in num_topics:\n",
    "        print('Building model for %s (%s topic)' % (c,i))\n",
    "        prefix = os.path.join(model_path, c, str(i), '')\n",
    "        os.makedirs(prefix, mode = out_path_mode, exist_ok = True)\n",
    "        cmallet[c][i] = gensim.models.wrappers.ldamallet.LdaMallet(mallet_path, corpora[c], id2word=dictionary, optimize_interval=10,\n",
    "                                                            prefix=prefix, workers=model_build_workers,\n",
    "                                                                   num_topics=i, iterations=2500, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save cmallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/topics-abstracts-2020-04-10-v7-scispacy/mallet_models/gs_models/2020-04-10-v7-covid19-combined-abstracts-gensim-mallet-model_text-ents-en_80.pkl4\n"
     ]
    }
   ],
   "source": [
    "for c in cmallet.keys():\n",
    "    for i in cmallet[c].keys():\n",
    "        cmallet[c][i].save(f'{gs_model_path_prefix}gensim-mallet-model_{c}_{i}.pkl4', \n",
    "                                    separately=[], sep_limit=134217728, pickle_protocol=4)\n",
    "        print(f'{gs_model_path_prefix}gensim-mallet-model_{c}_{i}.pkl4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:223: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
      "/opt/conda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:240: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lift = np.log(topic_term_dists / term_proportion)\n",
      "/opt/conda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:241: RuntimeWarning: divide by zero encountered in log\n",
      "  log_ttd = np.log(topic_term_dists)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/pyldavis_text-ents-en_80.json\n",
      "../results/2020-04-10-v7/topics-abstracts-2020-04-10-v7-scispacy-html/text-ents-en-80/pyldavis_text-ents-en_80.html\n"
     ]
    }
   ],
   "source": [
    "vis_data = {}\n",
    "gensim_lda_model = {}\n",
    "for c in cmallet.keys():\n",
    "    vis_data[c] = {}\n",
    "    gensim_lda_model[c] = {}\n",
    "    for i in cmallet[c].keys():\n",
    "        gensim_lda_model[c][i] = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(cmallet[c][i])\n",
    "        vis_data[c][i] = pyLDAvis.gensim.prepare(gensim_lda_model[c][i], corpora[c], \n",
    "                                                   dictionary=cmallet[c][i].id2word, mds='tsne')\n",
    "        pyLDAvis.save_json(vis_data[c][i], outdir + f'pyldavis_{c}_{i}.json')\n",
    "        print(outdir + f'pyldavis_{c}_{i}.json')\n",
    "        ofdir = web_out_dir + f'{c}-{i}/'\n",
    "        os.makedirs(ofdir, mode = out_path_mode, exist_ok = True)   \n",
    "        pyLDAvis.save_html(vis_data[c][i], ofdir + f'pyldavis_{c}_{i}.html',\n",
    "                          ldavis_url=MODIFIED_LDAVIS_URL)\n",
    "        print(web_out_dir + f'{c}-{i}/pyldavis_{c}_{i}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Gensim Mallet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/topics-abstracts-2020-04-10-v7-scispacy/mallet_models/gs_models/2020-04-10-v7-covid19-combined-abstracts-gensim-lda-model_text-ents-en_80.pkl4\n"
     ]
    }
   ],
   "source": [
    "for c in gensim_lda_model.keys():\n",
    "    for i in gensim_lda_model[c].keys():\n",
    "        gensim_lda_model[c][i].save(f'{gs_model_path_prefix}gensim-lda-model_{c}_{i}.pkl4', \n",
    "                                    separately=[], sep_limit=134217728, pickle_protocol=4)\n",
    "        print(f'{gs_model_path_prefix}gensim-lda-model_{c}_{i}.pkl4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save _Relevant_ terms for topics (from pyLDAviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_terms = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_terms(data, topic=1, rlambda=1, num_terms=30):\n",
    "    \"\"\"Returns a dataframe using lambda to calculate term relevance of a given topic.\"\"\"\n",
    "    tdf = pd.DataFrame(data.topic_info[data.topic_info.Category == 'Topic' + str(topic)])\n",
    "    if rlambda < 0 or rlambda > 1:\n",
    "        rlambda = 1\n",
    "    stdf = tdf.assign(relevance=rlambda * tdf['logprob'] + (1 - rlambda) * tdf['loglift'])\n",
    "    rdf = stdf[['Term', 'relevance']]\n",
    "    if num_terms:\n",
    "        return rdf.sort_values('relevance', ascending=False).head(num_terms).set_index(['Term'])\n",
    "    else:\n",
    "        return rdf.sort_values('relevance', ascending=False).set_index(['Term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lists = {}\n",
    "for corp, cdict in vis_data.items():\n",
    "    for numtops in cdict.keys():\n",
    "        model_topic_lists_dict = {}\n",
    "        for topnum in range(numtops):\n",
    "            s = sorted_terms(vis_data[corp][numtops], topnum + 1, rlambda=.5, num_terms=num_terms)\n",
    "            terms = s.index\n",
    "            model_topic_lists_dict['Topic ' + str(topnum + 1)] = np.pad(terms, (0, num_terms - len(terms)),\n",
    "                                                                               'constant', constant_values='')\n",
    "        topic_lists[corp + '-' + str(numtops)] = pd.DataFrame(model_topic_lists_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text-ents-en-80'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_lists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/topics-relevant-words-abstracts-2020-04-10-v7-50terms.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save relevant topics - write to xlsx (one corp-numtopics per sheet)\n",
    "with pd.ExcelWriter(outdir + f'topics-relevant-words-abstracts-{datafile_date}-{num_terms}terms.xlsx') as writer: \n",
    "    for sheetname, dataframe in topic_lists.items():\n",
    "        dataframe.to_excel(writer, sheet_name=sheetname)\n",
    "print(outdir + f'topics-relevant-words-abstracts-{datafile_date}-{num_terms}terms.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Relevant Topics as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/topics-abstracts-2020-04-10-v7-scispacy-html/text-ents-en-80/relevant_terms.html\n"
     ]
    }
   ],
   "source": [
    "# Save relevant topics - write to html\n",
    "out_topics_html_dir = web_out_dir\n",
    "for corp_numtopics, dataframe in topic_lists.items():\n",
    "    os.makedirs(out_topics_html_dir + corp_numtopics, mode = out_path_mode, exist_ok = True)\n",
    "    ofname = out_topics_html_dir + corp_numtopics + '/' + 'relevant_terms.html'\n",
    "    with open(ofname, 'w') as ofp:\n",
    "        column_tags = [f'<a href=\"Topic_{i+1:02d}.html\" target=\"_blank\">{name}</a>' \n",
    "                       for i, name in enumerate(dataframe.columns)]\n",
    "        temp_df = dataframe.copy()\n",
    "        temp_df.columns = column_tags\n",
    "        temp_df = temp_df.applymap(lambda x: ' '.join(x.split('_')))\n",
    "        temp_df = temp_df.set_index(np.arange(1, len(temp_df) + 1))\n",
    "        html_table = temp_df.to_html(escape=False)\n",
    "        html_str = html_template.format('Most Relevant Terms per Topic', html_style_cols, html_table)\n",
    "        ofp.write(html_str)\n",
    "    print(ofname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# topic_lists['text-ents-80']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes of topic model collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctopicwords_df = {}\n",
    "for c in cmallet.keys():\n",
    "    ctopicwords_df[c] = {}\n",
    "    for i in cmallet[c].keys():\n",
    "        ctopicwords_df[c][i] = pd.read_table(cmallet[c][i].ftopickeys(), header=None, names=['id', 'weight', 'wordlist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVED = []\n",
    "def normalize_topic_words(words):\n",
    "    results = []\n",
    "    for w in words:\n",
    "        if w in nonnumeric_punctuation:\n",
    "            pass\n",
    "        elif w[-1] == 's' and w[:-1] in words:\n",
    "            # remove plural\n",
    "            REMOVED.append(w)\n",
    "        elif w != w.lower() and w.lower() in words:\n",
    "            # remove capitalized\n",
    "            REMOVED.append(w)\n",
    "        else:\n",
    "            results.append(w)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean words\n",
    "for c in ctopicwords_df.keys():\n",
    "    for i in ctopicwords_df[c].keys():\n",
    "        ctopicwords_df[c][i]['wordlist'] = ctopicwords_df[c][i]['wordlist'].apply(lambda x: ' '.join(normalize_topic_words(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(REMOVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ctopicwords_df.keys():\n",
    "    for i in ctopicwords_df[c].keys():\n",
    "        ctopicwords_df[c][i].drop(['id'], axis=1, inplace=True)\n",
    "        ctopicwords_df[c][i]['topwords'] = ctopicwords_df[c][i].wordlist.apply(lambda x: ' '.join(x.split()[:3]))\n",
    "        ctopicwords_df[c][i]['topten'] = ctopicwords_df[c][i].wordlist.apply(lambda x: ' '.join(x.split()[:10]))\n",
    "        if True:  # use pyLDAvis order\n",
    "            rank_order_new_old = vis_data[c][i].to_dict()['topic.order']\n",
    "            rank_order_old_new = [None] * len(rank_order_new_old)\n",
    "            for new, old in enumerate(rank_order_new_old):\n",
    "                rank_order_old_new[old - 1] = new\n",
    "            ctopicwords_df[c][i]['rank'] = np.array(rank_order_old_new) + 1\n",
    "        else:\n",
    "            ctopicwords_df[c][i]['rank'] = ctopicwords_df[c][i].weight.rank(ascending=False)\n",
    "        ctopicwords_df[c][i]['topicnum'] = ctopicwords_df[c][i].apply(lambda row: ('t%02d' % row['rank']), axis=1)\n",
    "        ctopicwords_df[c][i]['label'] = ctopicwords_df[c][i].apply(lambda row: row['topicnum'] + ' ' + row['topwords'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.045221</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.025943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.079086</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.167942</td>\n",
       "      <td>0.137487</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.175804</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.033891</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.033923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.030367</td>\n",
       "      <td>0.109234</td>\n",
       "      <td>0.103199</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000466  0.000177  0.000206  0.000070  0.045221  0.000129  0.000081   \n",
       "1  0.001900  0.000723  0.000840  0.079086  0.000327  0.000526  0.000330   \n",
       "2  0.000276  0.007737  0.015385  0.000042  0.000048  0.000076  0.000048   \n",
       "3  0.000489  0.000186  0.000216  0.000074  0.000084  0.000135  0.000085   \n",
       "4  0.000438  0.000167  0.000194  0.000066  0.000075  0.000121  0.000076   \n",
       "\n",
       "         7         8         9   ...        70        71        72        73  \\\n",
       "0  0.000086  0.000052  0.000076  ...  0.000135  0.000006  0.000080  0.000082   \n",
       "1  0.000350  0.000210  0.000308  ...  0.000550  0.000025  0.000326  0.000335   \n",
       "2  0.000051  0.000030  0.000045  ...  0.000080  0.000004  0.000047  0.167942   \n",
       "3  0.000090  0.000054  0.000079  ...  0.000141  0.000006  0.000084  0.000086   \n",
       "4  0.000081  0.000048  0.000071  ...  0.000127  0.000006  0.000075  0.030367   \n",
       "\n",
       "         74        75        76        77        78        79  \n",
       "0  0.000203  0.000227  0.000123  0.000118  0.000099  0.025943  \n",
       "1  0.000827  0.000923  0.000501  0.000479  0.000403  0.000605  \n",
       "2  0.137487  0.000134  0.000073  0.000070  0.000059  0.000088  \n",
       "3  0.175804  0.006991  0.000129  0.033891  0.006857  0.033923  \n",
       "4  0.109234  0.103199  0.000116  0.000111  0.000093  0.000139  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doctopics\n",
    "cdoctopics_df = {}\n",
    "for c in cmallet.keys():\n",
    "    cdoctopics_df[c] = {}\n",
    "    for n in cmallet[c].keys():\n",
    "        cdoctopics_df[c][n] = pd.read_table(cmallet[c][n].fdoctopics(), header=None, names=['id']+[i for i in range(n)])\n",
    "        cdoctopics_df[c][n].drop(['id'], axis=1, inplace=True)\n",
    "cdoctopics_df[c][n].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder topics\n",
    "for c in cdoctopics_df.keys():\n",
    "    for n in cdoctopics_df[c].keys():\n",
    "# (include top 3 topics in name)        cdoctopics_df[c][n] = cdoctopics_df[c][n].T.join(ctopicwords_df[c][n][['rank', 'label']]).set_index('label').sort_values('rank').drop(['rank'], axis=1).T\n",
    "        cdoctopics_df[c][n] = cdoctopics_df[c][n].T.join(ctopicwords_df[c][n][['rank', 'topicnum']]).set_index('topicnum').sort_values('rank').drop(['rank'], axis=1).T\n",
    "        cdoctopics_df[c][n].T.index.rename('topic', inplace=True)\n",
    "# cdoctopics_df[c][n].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/topickeys_sorted_text-ents-en_80.txt\n"
     ]
    }
   ],
   "source": [
    "# Save topicwords\n",
    "for c in ctopicwords_df.keys():\n",
    "    for i in ctopicwords_df[c].keys():\n",
    "        ctopicwords_df[c][i].sort_values('rank').to_csv(outdir + 'topickeys_sorted_%s_%d.txt' % (c, i), index_label='original_order')\n",
    "        print(outdir + 'topickeys_sorted_%s_%d.txt' % (c, i))\n",
    "        # ctopicwords_df[c][i].sort_values('rank').to_excel('out/topickeys_sorted_%s_%d.xlsx' % (c, i), index_label='original_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/doctopic_text-ents-en_80.csv\n"
     ]
    }
   ],
   "source": [
    "# Save doctopics\n",
    "for c in cdoctopics_df.keys():\n",
    "    for n in cdoctopics_df[c].keys():\n",
    "        cdoctopics_df[c][n].to_csv(outdir + 'doctopic_%s_%d.csv' % (c, n), index_label='original_order')\n",
    "        print(outdir + 'doctopic_%s_%d.csv' % (c, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_names = ['scispacy', 'specter']\n",
    "sims_columns = [f'sims_{x}_cord_uid' for x in sims_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(x in original_df.columns for x in sims_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'cord_uid' in original_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_get_sims_html_ids(sim_uids, cord_uid_topic_num, cord_uid_cite_ad):\n",
    "    result = []\n",
    "    for uid in sim_uids:\n",
    "        topic_num = cord_uid_topic_num.get(uid)\n",
    "        cite_ad = cord_uid_cite_ad.get(uid)\n",
    "        if cite_ad and topic_num:\n",
    "            result.append(f'<a href=\"Topic_{topic_num}.html#{uid}\">{cite_ad}</a>')\n",
    "    return ', '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df['abstract_mentions_covid'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to save docs by topics\n",
    "predominant_doc_dfd = {}\n",
    "predominant_doc_df = original_df[['cite_ad', 'title', 'authors', 'publish_year', 'publish_time', \n",
    "                                  'dataset', 'abstract_mentions_covid',\n",
    "                                 'pmcid', 'pubmed_id', 'doi', 'cord_uid', 'sha', 'abstract_clean']\n",
    "                                 + sims_columns\n",
    "                                 ].copy()\n",
    "sims_mapping_cord_uid_sd = {}\n",
    "predominant_doc_df['publish_time'] = predominant_doc_df['publish_time'].dt.strftime('%Y-%m-%d')\n",
    "for c in cdoctopics_df.keys():\n",
    "    predominant_doc_dfd[c] = {}\n",
    "    sims_mapping_cord_uid_sd[c] = {}\n",
    "    for n in cdoctopics_df[c].keys():\n",
    "        predominant_doc_dfd[c][n] = {}\n",
    "        sims_mapping_cord_uid_sd[c][n] = {}\n",
    "        predominant_doc_df['predominant_topic'] = cdoctopics_df[c][n].idxmax(axis=1)\n",
    "        predominant_doc_df['predominant_topic_num'] = predominant_doc_df['predominant_topic'].str.split().apply(lambda x: x[0][1:])\n",
    "        predominant_doc_df['major_topics'] = cdoctopics_df[c][n].apply(lambda r: {f't{i + 1:02d}': val for i, val in enumerate(r) if val >= 0.3}, axis=1)\n",
    "        for sim_col in sims_columns:\n",
    "            sims_mapping_cord_uid_sd[c][n][sim_col] = {}\n",
    "            sims_mapping_cord_uid_sd[c][n][sim_col]['topic_num'] = predominant_doc_df[['cord_uid', 'predominant_topic_num']].set_index('cord_uid')['predominant_topic_num']\n",
    "            sims_mapping_cord_uid_sd[c][n][sim_col]['cite_ad'] = predominant_doc_df[['cord_uid', 'cite_ad']].set_index('cord_uid')['cite_ad']\n",
    "        for i, topic_name in enumerate(cdoctopics_df[c][n].columns):        \n",
    "            temp_df = predominant_doc_df[(predominant_doc_df['major_topics'].apply(lambda x: topic_name in x))].copy()\n",
    "            temp_df['topic_weight'] = temp_df.major_topics.apply(lambda x: x.get(topic_name))\n",
    "            temp_df = temp_df.sort_values(['topic_weight'], axis=0, ascending=False)\n",
    "            predominant_doc_dfd[c][n][i] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/topics-central-docs-abstracts-2020-04-10-v7-text-ents-en-80.{jsonl, txt}\n"
     ]
    }
   ],
   "source": [
    "# Save docs by topics - write to json and tsv\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        outfile_central_docs_base = outdir + f'topics-central-docs-abstracts-{datafile_date}-{c}-{n}'\n",
    "        temp_dfs = []\n",
    "        for i, dataframe in predominant_doc_dfd[c][n].items():\n",
    "            temp_df = dataframe[['title', 'authors', 'publish_year', 'publish_time', 'cord_uid', 'dataset', 'sha', 'abstract_clean']].reset_index()\n",
    "            temp_df['Topic'] = i + 1\n",
    "            temp_dfs.append(temp_df)\n",
    "        result_df = pd.concat(temp_dfs)\n",
    "        print(outfile_central_docs_base + '.{jsonl, txt}')\n",
    "        result_df.to_json(outfile_central_docs_base + '.jsonl', **out_json_args)\n",
    "        result_df.to_csv(outfile_central_docs_base + '.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/topics-central-docs-abstracts-2020-04-10-v7-text-ents-en-80.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save docs by topics - write to excel\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        print(outdir + f'topics-central-docs-abstracts-{datafile_date}-{c}-{n}.xlsx')\n",
    "        with pd.ExcelWriter(outdir + f'topics-central-docs-abstracts-{datafile_date}-{c}-{n}.xlsx') as writer: \n",
    "            for i in predominant_doc_dfd[c][n].keys():\n",
    "                sheetname = f'Topic {i+1}'\n",
    "                predominant_doc_dfd[c][n][i].drop(columns=['abstract_clean', 'cite_ad', 'major_topics',\n",
    "                                                          'predominant_topic', 'predominant_topic_num']\n",
    "                                                 ).to_excel(writer, sheet_name=sheetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep similarity columns for html\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        for sim_name, sims_col in zip(sims_names, sims_columns):\n",
    "            cord_uid_topic_num = sims_mapping_cord_uid_sd[c][n][sim_col]['topic_num'].to_dict()\n",
    "            cord_uid_cite_ad = sims_mapping_cord_uid_sd[c][n][sim_col]['cite_ad'].to_dict()\n",
    "            for i in predominant_doc_dfd[c][n].keys():\n",
    "                predominant_doc_dfd[c][n][i][f'Similarity {sim_name}'] = (predominant_doc_dfd[c][n][i][sims_col]\n",
    "                 .apply(lambda x: helper_get_sims_html_ids(x, cord_uid_topic_num, cord_uid_cite_ad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for html\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        for i in predominant_doc_dfd[c][n].keys():\n",
    "            predominant_doc_dfd[c][n][i]['pmcid'] = predominant_doc_dfd[c][n][i]['pmcid'].apply(lambda xid: f'<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/{xid}\" target=\"_blank\">{xid}</a>' if not pd.isnull(xid) else '')\n",
    "            predominant_doc_dfd[c][n][i]['pubmed_id'] = predominant_doc_dfd[c][n][i]['pubmed_id'].apply(lambda xid: f'<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/{xid}\" target=\"_blank\">{xid}</a>' if not pd.isnull(xid) else '')\n",
    "            predominant_doc_dfd[c][n][i]['doi'] = predominant_doc_dfd[c][n][i]['doi'].apply(lambda xid: f'<a href=\"https://doi.org/{xid}\" target=\"_blank\">{xid}</a>' if not pd.isnull(xid) else '')\n",
    "            predominant_doc_dfd[c][n][i]['abstract_mentions_covid'] = predominant_doc_dfd[c][n][i]['abstract_mentions_covid'].apply(lambda x: 'Y' if x else 'N')\n",
    "            predominant_doc_dfd[c][n][i].columns = [' '.join(c.split('_')) for c in predominant_doc_dfd[c][n][i].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.formats import format as fmt\n",
    "from pandas.io.formats.html import HTMLFormatter\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLFormatter(HTMLFormatter):\n",
    "    \"Add html id to th for rows\"\n",
    "    def __init__(self, html_id_col_name, *args, **kwargs):\n",
    "        super(MyHTMLFormatter, self).__init__(*args, **kwargs)\n",
    "        self.html_id_col_name = html_id_col_name\n",
    "\n",
    "    def write_th(\n",
    "        self, s: Any, header: bool = False, indent: int = 0, tags: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if not header and self.html_id_col_name and self.html_id_col_name in self.frame.columns:\n",
    "            try:\n",
    "                key = int(s.strip())\n",
    "            except ValueError:\n",
    "                key = None\n",
    "            if key and key in self.frame.index:\n",
    "                html_id = self.frame.loc[key, self.html_id_col_name]\n",
    "                if html_id:\n",
    "                    if tags:\n",
    "                        tags += f'id=\"{html_id}\";'\n",
    "                    else:\n",
    "                        tags = f'id=\"{html_id}\";'\n",
    "        super(MyHTMLFormatter, self).write_th(s, header, indent, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/2020-04-10-v7/topics-abstracts-2020-04-10-v7-scispacy-html/text-ents-en-80/\n"
     ]
    }
   ],
   "source": [
    "# Save doc by topics - write to html\n",
    "# out_topics_html_dir = outdir + f'topics-central-docs-abstracts-{datafile_date}-html/'\n",
    "out_topics_html_dir = web_out_dir\n",
    "os.makedirs(out_topics_html_dir, mode = out_path_mode, exist_ok = True)\n",
    "for c in predominant_doc_dfd.keys():\n",
    "    for n in predominant_doc_dfd[c].keys():\n",
    "        ofdir = out_topics_html_dir + f'{c}-{n}/'\n",
    "        os.makedirs(ofdir, mode = out_path_mode, exist_ok = True)   \n",
    "        print(ofdir)\n",
    "        for i in predominant_doc_dfd[c][n].keys():\n",
    "            ofname = ofdir + f'Topic_{i+1:02d}.html'\n",
    "            with open(ofname, 'w') as ofp:\n",
    "                html_df = (predominant_doc_dfd[c][n][i]\n",
    "                                .drop(columns=['sha', 'major topics', 'abstract clean',\n",
    "                                              'predominant topic', 'predominant topic num'] \n",
    "                                      + [' '.join(c.split('_')) for c in sims_columns])\n",
    "                                .copy()\n",
    "                                .set_index(np.arange(1, len(predominant_doc_dfd[c][n][i])+1)))\n",
    "                # html_table = html_df.to_html(escape=False)\n",
    "                df_formatter = fmt.DataFrameFormatter(escape=False, frame=html_df, index=True, bold_rows=True)\n",
    "                html_formatter = MyHTMLFormatter('cord uid', formatter=df_formatter)\n",
    "                # html_formatter = HTMLFormatter(formatter=df_formatter)\n",
    "                html_table = html_formatter.get_result()\n",
    "                html_str = html_template.format(f'Topic {i+1:02d}', html_style, html_table)\n",
    "                ofp.write(html_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
